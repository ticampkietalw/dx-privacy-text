{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance to neighbors w/ different GloVe Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import numpy as np\n",
    "from cupyx.scipy.spatial import distance\n",
    "import cupy as cp\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# Add the main directory to sys.path to be able to import config\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from config import ROOT_DIR\n",
    "from utils.dx import sample_noise_vectors\n",
    "from utils.tools import compute_distances, argsort_chunked\n",
    "\n",
    "# PARAMS\n",
    "distance_metric = \"euclidean\"\n",
    "distances_dtype = np.float16 # Precision of the distances\n",
    "\n",
    "glove_variant = \"6B\" #\"Twitter\" or \"6B\"\n",
    "\n",
    "glove_data_folderpath = ROOT_DIR\n",
    "# END PARAMS\n",
    "\n",
    "if glove_variant == \"6B\":\n",
    "    glove_dimension_to_filename = {\n",
    "        50: \"glove.6B.50d.pkl\", # 400000 words\n",
    "        100:\"glove.6B.100d.pkl\", # 400000 words\n",
    "        200: \"glove.6B.200d.pkl\", # 400000 words\n",
    "        300:\"glove.6B.300d.pkl\" # 400000 words\n",
    "    }\n",
    "elif glove_variant == \"Twitter\":\n",
    "    glove_dimension_to_filename = {\n",
    "        25: \"glove.twitter.27B.25d.pkl\", #1,193,513 words\n",
    "        50: \"glove.twitter.27B.50d.pkl\", #1,193,513 words\n",
    "        100: \"glove.twitter.27B.100d.pkl\", #1,193,513 words\n",
    "        200: \"glove.twitter.27B.200d.pkl\", #1,193,513 words\n",
    "    }\n",
    "fit_dtype = np.uint32 #dtype fit to encode the number of words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance to two arbitrary neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics will be averaged for this number of words\n",
    "sample_size = 5000\n",
    "hidden_sizes = list(glove_dimension_to_filename.keys())\n",
    "x_rank = 1 # Rank of neighbor x\n",
    "y_rank = 2 # Rank of neighbor y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell processes each dimension of the selected *glove_variant* in parallel. For each parallel process:\n",
    "- Load GloVe for the selected dimension\n",
    "- Take *sample_size* words and compute the distances against the entire vocabulary\n",
    "- For each word, rank their neighbors\n",
    "- For each word, compute the distance between their *x*-th and *y*-th neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dimension(hidden_size):\n",
    "    # Load GloVe vocabulary and store it into suitable structures\n",
    "    with open(join(glove_data_folderpath, glove_dimension_to_filename[hidden_size]), \"rb\") as f:\n",
    "        glove = pickle.load(f)\n",
    "\n",
    "    vocab_embs = cp.array(list(glove.values())) # Put on GPU\n",
    "    vocab_size = vocab_embs.shape[0]\n",
    "    del glove # Save RAM\n",
    "\n",
    "    # Take sample_size words and compute the distances against the entire vocabulary\n",
    "    words_ids = np.random.randint(0, vocab_size, size=sample_size)\n",
    "    words_embeddings = vocab_embs[words_ids]\n",
    "\n",
    "    # Step by step distance computation for the selected words\n",
    "    distances = compute_distances(words_embeddings, vocab_embs, distance_metric, dtype=distances_dtype)\n",
    "    \n",
    "    # For each word, get a sorted list of their neighbors.\n",
    "    word_neighbors = argsort_chunked(distances, fit_dtype)\n",
    "\n",
    "    # For each word, get a sorted list of the distances with the entire vocabulary.\n",
    "    # Instead of sorting again, benefit from word_neighbors.\n",
    "    sorted_distances = np.take_along_axis(distances, word_neighbors, axis=-1)\n",
    "\n",
    "    # Distance to the neighbor x\n",
    "    distances_to_x = sorted_distances[:, x_rank]\n",
    "\n",
    "    # Distance to the neighbor y\n",
    "    distances_to_y = sorted_distances[:, y_rank]\n",
    "\n",
    "    # Gather the ids of x and y\n",
    "    x_neighbors = word_neighbors[:, x_rank:x_rank+1]\n",
    "    y_neighbors = word_neighbors[:, y_rank:y_rank+1]\n",
    "    x_and_y_neighbors = np.concatenate((x_neighbors, y_neighbors), axis=1)\n",
    "\n",
    "    # Compute the distance between x and y\n",
    "    distances_between_x_and_y = np.empty((sample_size), dtype=distances_dtype)\n",
    "    for i in range(sample_size):\n",
    "        # Using cdist because cupyx.scipy.spatial.distance.euclidean has a bug https://github.com/cupy/cupy/issues/8288\n",
    "        distances_between_x_and_y[i] = distance.cdist(vocab_embs[x_and_y_neighbors[i][0]:x_and_y_neighbors[i][0]+1], vocab_embs[x_and_y_neighbors[i][1]:x_and_y_neighbors[i][1]+1], distance_metric).item()\n",
    "\n",
    "    return (distances_to_x, distances_to_y, distances_between_x_and_y)\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=5) as executor:\n",
    "    results = list(executor.map(process_dimension, hidden_sizes))\n",
    "\n",
    "distances_to_x = dict(zip(hidden_sizes, [results[i][0] for i in range(len(hidden_sizes))]))\n",
    "distances_to_y = dict(zip(hidden_sizes, [results[i][1] for i in range(len(hidden_sizes))]))\n",
    "distances_between_x_and_y = dict(zip(hidden_sizes, [results[i][2] for i in range(len(hidden_sizes))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_size in hidden_sizes:\n",
    "    print(f\"{glove_variant}-{hidden_size}d Average half distance to neighbor {x_rank} = {distances_to_x[hidden_size].mean()/2:.3f}\")\n",
    "    print(f\"{glove_variant}-{hidden_size}d Average half distance to neighbor {y_rank} = {distances_to_y[hidden_size].mean()/2:.3f}\")\n",
    "    print(f\"{glove_variant}-{hidden_size}d Average eq19 for neighbor {x_rank} and {y_rank} = {(((distances_to_y[hidden_size]**2)-(distances_to_x[hidden_size]**2))/(2*distances_between_x_and_y[hidden_size])).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounds on $\\Pr[C_i]$\n",
    "Let $C_{i}$ be the event that word embedding $\\mathbf{x}_i$ is the nearest neighbor of $\\mathbf{w}^*$. From equations following the Theorem 10, we have the following bounds:\n",
    "\n",
    "$\\Pr[C_i] \\leq 1 - \\Pr\\left[ Z \\leq \\frac{\\lVert{\\mathbf{w} - \\mathbf{x}_1}\\rVert} {2}\\right]$ (Eq 20 in the paper) and $\\Pr[C_i] \\leq \\Pr \\left[ Z \\leq \\frac{\\lVert{\\mathbf{w} - \\mathbf{x}_j}\\rVert^2 - \\lVert{\\mathbf{w} - \\mathbf{x}_i}\\rVert^2  }{2 \\lVert{\\mathbf{x}_i - \\mathbf{x}_j}\\rVert}\\right] $ (Eq 21 in the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "Get $\\Pr\\left[ Z \\leq \\frac{\\lVert{\\mathbf{w} - \\mathbf{x}_1}\\rVert} {2}\\right]$ for different epsilons and dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics will be averaged for this number of words\n",
    "sample_size = 5000\n",
    "hidden_sizes = list(glove_dimension_to_filename.keys())\n",
    "epsilons = [i for i in range(1, 101)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dimension(hidden_size):\n",
    "    # Load GloVe vocabulary and store it into suitable structures\n",
    "    with open(join(glove_data_folderpath, glove_dimension_to_filename[hidden_size]), \"rb\") as f:\n",
    "        glove = pickle.load(f)\n",
    "\n",
    "    vocab_embs = cp.array(list(glove.values())) # Put on GPU\n",
    "    vocab_size = vocab_embs.shape[0]\n",
    "    del glove # Save RAM\n",
    "\n",
    "    # Take sample_size words and compute the distances against the entire vocabulary\n",
    "    words_ids = np.random.randint(0, vocab_size, size=sample_size)\n",
    "    words_embeddings = vocab_embs[words_ids]\n",
    "\n",
    "    # Step by step distance computation for the selected words\n",
    "    distances = compute_distances(words_embeddings, vocab_embs, distance_metric, dtype=distances_dtype)\n",
    "\n",
    "    # For each word, get a sorted list of their neighbors.\n",
    "    word_neighbors = distances.argsort(axis=-1).astype(fit_dtype)\n",
    "\n",
    "    # For each word, get a sorted list of the distances with the entire vocabulary.\n",
    "    # Instead of sorting again, benefit from word_neighbors.\n",
    "    # Doing distances[word_neighbors] here would not work as word_neighbors is a 2D array and would\n",
    "    # result in numpy advanced indexing\n",
    "    sorted_distances = np.take_along_axis(distances, word_neighbors, axis=-1)\n",
    "\n",
    "    # Get the distance with the first neighbor of each word\n",
    "    distance_with_1st_neighbor = sorted_distances[:, 1]\n",
    "\n",
    "    proportions = {}\n",
    "    for epsilon in epsilons:\n",
    "        # Noise the words\n",
    "        noises = sample_noise_vectors(dimension=hidden_size,\n",
    "                                            shape1=1,\n",
    "                                            shape2=sample_size,\n",
    "                                            epsilon=epsilon)[0]\n",
    "\n",
    "        # z = rk by definition\n",
    "        # r is the norm of the noise\n",
    "        r = np.linalg.norm(noises, ord=None, axis=-1)\n",
    "        # k is the cosine angle between the noise and the word w.\n",
    "        # This is computed here as one minus the cosine distance between said vectors.\n",
    "        k = 1 - np.array([distance.cdist([noises[i]], [words_embeddings[i]], \"cosine\").astype(distances_dtype).item() for i in range(sample_size)])\n",
    "\n",
    "        # Formula of Equation 18 in the paper\n",
    "        formula = (r*k) <= 1/2 * distance_with_1st_neighbor\n",
    "        proportions[epsilon] = formula.mean()\n",
    "\n",
    "    return proportions\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=5) as executor:\n",
    "    resultsPart1 = list(executor.map(process_dimension, hidden_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are stored in *resultsPart1*, which is a list each item holds the result for one vocabulary dimension, in the same order as *hidden_sizes* variable. Each item of *resultsPart1* is a dictionary where the keys are integers representing the value of epsilon. The dictionary associates each epsilon with a float representing the empirical probability $\\Pr\\left[ Z \\leq \\frac{\\lVert{\\mathbf{w} - \\mathbf{x}_1}\\rVert} {2}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "Get $\\Pr \\left[ Z \\leq \\frac{\\lVert{\\mathbf{w} - \\mathbf{x}_j}\\rVert^2 - \\lVert{\\mathbf{w} - \\mathbf{x}_i}\\rVert^2  }{2 \\lVert{\\mathbf{x}_i - \\mathbf{x}_j}\\rVert}\\right]$ for\n",
    "- 1st and 2nd neighbors\n",
    "- 1st and 101th neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics will be averaged for this number of words\n",
    "sample_size = 10\n",
    "hidden_sizes = list(glove_dimension_to_filename.keys())\n",
    "epsilons = [i for i in range(1, 101)]\n",
    "\n",
    "# X and Y pairs to test\n",
    "x_ranks = [1,1]\n",
    "y_ranks = [2,101]\n",
    "assert len(x_ranks) == len(y_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the formula in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dimension(hidden_size):\n",
    "    # Load GloVe vocabulary and store it into suitable structures\n",
    "    with open(join(glove_data_folderpath, glove_dimension_to_filename[hidden_size]), \"rb\") as f:\n",
    "        glove = pickle.load(f)\n",
    "\n",
    "    vocab_embs = cp.array(list(glove.values())) # Put on GPU\n",
    "    vocab_size = vocab_embs.shape[0]\n",
    "    del glove # Save RAM\n",
    "\n",
    "    # Take sample_size words and compute the distances against the entire vocabulary\n",
    "    words_ids = np.random.randint(0, vocab_size, size=sample_size)\n",
    "    words_embeddings = vocab_embs[words_ids]\n",
    "\n",
    "    # Step by step distance computation for the selected words\n",
    "    distances = compute_distances(words_embeddings, vocab_embs, distance_metric, dtype=distances_dtype)\n",
    "\n",
    "    # For each word, get a sorted list of their neighbors.\n",
    "    word_neighbors = distances.argsort(axis=-1).astype(fit_dtype)\n",
    "\n",
    "    # For each word, get a sorted list of the distances with the entire vocabulary.\n",
    "    # Instead of sorting again, benefit from word_neighbors.\n",
    "    sorted_distances = np.take_along_axis(distances, word_neighbors, axis=-1)\n",
    "\n",
    "    results = []\n",
    "    # For all x and y pairs defined above\n",
    "    for i in range(len(x_ranks)):\n",
    "        x_rank = x_ranks[i]\n",
    "        y_rank = y_ranks[i]\n",
    "        # Distance to the neighbor x\n",
    "        distances_to_x = sorted_distances[:, x_rank]\n",
    "\n",
    "        # Distance to the neighbor y\n",
    "        distances_to_y = sorted_distances[:, y_rank]\n",
    "\n",
    "        # Compute the distance between the word and x, between the word and y, and between x and y\n",
    "        x_neighbors = word_neighbors[:, x_rank:x_rank+1]\n",
    "        y_neighbors = word_neighbors[:, y_rank:y_rank+1]\n",
    "        x_and_y_neighbors = np.concatenate((x_neighbors, y_neighbors), axis=1)\n",
    "\n",
    "        distances_between_x_and_y = np.array([distance.cdist(vocab_embs[x_and_y_neighbors[i][0]:x_and_y_neighbors[i][0]+1], vocab_embs[x_and_y_neighbors[i][1]:x_and_y_neighbors[i][1]+1], distance_metric).astype(distances_dtype).item() for i in range(sample_size)], dtype=distances_dtype)\n",
    "\n",
    "        # Compute the formula defined above\n",
    "        proportions = {}\n",
    "        for epsilon in epsilons:\n",
    "            # Noise the words\n",
    "            noises = sample_noise_vectors(dimension=hidden_size,\n",
    "                                                shape1=1,\n",
    "                                                shape2=sample_size,\n",
    "                                                epsilon=epsilon)[0]\n",
    "\n",
    "            # z = rk by definition\n",
    "            # r is the norm of the noise\n",
    "            r = np.linalg.norm(noises, ord=None, axis=-1)\n",
    "            # k is the cosine angle between the noise and the word w.\n",
    "            # This is computed here as one minus the cosine distance between said vectors.\n",
    "            k = 1 - np.array([distance.cdist([noises[i]], [words_embeddings[i]], \"cosine\").astype(distances_dtype).item() for i in range(sample_size)])\n",
    "\n",
    "            # Formula of Equation 19 in the paper\n",
    "            formula = (r*k) <= ((distances_to_y**2)-(distances_to_x**2))/(2*distances_between_x_and_y)\n",
    "            proportions[epsilon] = formula.mean()\n",
    "        results.append(proportions)\n",
    "    return {\"results\":results, \"x_ranks\":x_ranks, \"y_ranks\":y_ranks, \"epsilons\":epsilons}\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=5) as executor:\n",
    "    resultsPart2 = list(executor.map(process_dimension, hidden_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are stored in *resultsPart2*, which is a list each item holds the result for one vocabulary dimension, in the same order as *hidden_sizes* variable. Each item of *resultsPart2* is a dictionary where the keys are integers representing the value of epsilon. The dictionary associates each epsilon with a float representing the empirical probability $\\Pr \\left[ Z \\leq \\frac{\\lVert{\\mathbf{w} - \\mathbf{x}_j}\\rVert^2 - \\lVert{\\mathbf{w} - \\mathbf{x}_i}\\rVert^2  }{2 \\lVert{\\mathbf{x}_i - \\mathbf{x}_j}\\rVert}\\right]$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dx-privacy-text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
