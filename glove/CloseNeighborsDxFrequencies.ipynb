{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Neighbors $d_\\mathcal{X}$-privacy frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# Add the main directory to sys.path to be able to import config\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from config import ROOT_DIR\n",
    "from utils.dx import sample_noise_vectors, noisy_embeddings_to_ids\n",
    "from utils.tools import rank_neighbors\n",
    "\n",
    "# PARAMS\n",
    "distance_metric = \"euclidean\" # Metric to use for the distances\n",
    "distances_dtype = np.float16 # Precision of the distances\n",
    "\n",
    "glove_variant = \"6B\" #\"Twitter\" or \"6B\"\n",
    "\n",
    "glove_data_folderpath = ROOT_DIR\n",
    "\n",
    "# END PARAMS\n",
    "if glove_variant == \"6B\":\n",
    "    glove_dimension_to_filename = {\n",
    "        50: \"glove.6B.50d.pkl\", # 400000 words\n",
    "        100:\"glove.6B.100d.pkl\", # 400000 words\n",
    "        200: \"glove.6B.200d.pkl\", # 400000 words\n",
    "        300:\"glove.6B.300d.pkl\" # 400000 words\n",
    "    }\n",
    "elif glove_variant == \"Twitter\":\n",
    "    glove_dimension_to_filename = {\n",
    "        25: \"glove.twitter.27B.25d.pkl\", #1,193,513 words\n",
    "        50: \"glove.twitter.27B.50d.pkl\", #1,193,513 words\n",
    "        100: \"glove.twitter.27B.100d.pkl\", #1,193,513 words\n",
    "        200: \"glove.twitter.27B.200d.pkl\", #1,193,513 words\n",
    "    }\n",
    "fit_dtype = np.uint32 # Integer size sufficient to encode the number of words in the vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average several words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load GloVe in a specific dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "with open(join(glove_data_folderpath, glove_dimension_to_filename[hidden_size]), \"rb\") as f:\n",
    "    glove = pickle.load(f)\n",
    "\n",
    "vocab_embs = np.array(list(glove.values()))\n",
    "vocab_size = vocab_embs.shape[0]\n",
    "del glove # Save RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select *number_of_words* random words and rank their neighbors according to their distance with the word in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = 5000\n",
    "words_ids = np.random.randint(0, vocab_size, size=number_of_words)\n",
    "words_embs = vocab_embs[words_ids]\n",
    "\n",
    "words_neighbors_ranked = rank_neighbors(words_embs, vocab_embs, distance_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add noise to the embeddings of the words following the $d_x$-privacy mechanism and count which neighbor was chosen, represented by its rank in the neighbor list of the initial word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [i for i in range(1, 31)] # Parameter\n",
    "neighbor_counted_occurences = {}\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    embeddings = np.copy(words_embs)\n",
    "    noise = sample_noise_vectors(dimension=hidden_size,\n",
    "                                        shape1=1,\n",
    "                                        shape2=number_of_words,\n",
    "                                        epsilon=epsilon)[0]\n",
    "    # Adding noise to embeddings\n",
    "    noisy_embeddings = embeddings + noise\n",
    "\n",
    "    # Convert embedding back to text via Nearest neighbor\n",
    "    noisy_word_ids = noisy_embeddings_to_ids(noisy_embeddings, vocab_embs, distance_metric)\n",
    "\n",
    "    # for all words_ids, get the rank k of noisy_word_ids[i] and increase a counter at index k\n",
    "    noisy_word_ids_ranks = words_neighbors_ranked[np.arange(number_of_words), noisy_word_ids] # This line, for all the elements i in the first dimension of words_neighbors_ranked, gets the particular value pointed by the index which is stored at noisy_word_ids[i]\n",
    "    noisy_word_ids_ranks_counted = Counter(noisy_word_ids_ranks)\n",
    "    neighbor_counted_occurences[epsilon] = [noisy_word_ids_ranks_counted[k] for k in range(vocab_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are stored in *neighbor_counted_occurences*, which is a dictionary where the keys are integers representing the value of epsilon. The dictionary associates each epsilon with a list, where list[i] contains the number of times the i-th neighbor was chosen as the replacement of a word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average several words (with post-processing fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-processing step after we have found the nearest word $\\mathbf{x}^*$ to the noisy embedding $\\mathbf{w}^*$. We sort the nearest neighbors of $\\mathbf{x}^*$ and output a neighbor proportional to $\\exp(- d_\\text{NN}(\\mathbf{x}^*, \\mathbf{x}))$. More specifically any word $\\mathbf{x} \\in \\mathcal{D}$ is output with probability:\n",
    "$$\\frac{\\exp(- c \\epsilon d_\\text{NN}(\\mathbf{x}^*, \\mathbf{x}))}{\\sum_{\\mathbf{x} \\in \\mathcal{D}} \\exp(- c \\epsilon d_\\text{NN}(\\mathbf{x}^*, \\mathbf{x}))}, \n",
    "$$\n",
    "where $c$ is a constant to control how many neighbors are likely to be selected. A higher value such as $c > 1$ means that the mechanism will output the first few neighbors with high probability, and a lower value such as $c = 0.01$ means that more neighbors will likely to be output, of course, with probability exponentially decreasing as we move away from the original word. This is the same as the temperature variable in the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new nearest neighbor search function.\n",
    "# It requires an already computed matrix of ranked neighbors.\n",
    "# Used for faster tests of different dx_constant for the same embeddings without \n",
    "# having to recompute the ranks of the neighbors.\n",
    "def noisy_embeddings_to_ids_dxfix_lazy(\n",
    "        words_embeddings: np.ndarray,\n",
    "        vocabulary: np.ndarray,\n",
    "        neighbors_ranked: np.ndarray,\n",
    "        dx_constant: int,\n",
    "        epsilon: int\n",
    "    ) -> np.ndarray:\n",
    "    number_of_words = words_embeddings.shape[0]\n",
    "    vocab_size = vocabulary.shape[0]\n",
    "\n",
    "    probabilities = np.exp(-dx_constant*epsilon*neighbors_ranked)\n",
    "\n",
    "    probabilities_summed = probabilities.sum(axis=-1, keepdims=True)\n",
    "    probabilities = probabilities / probabilities_summed\n",
    "\n",
    "    noisy_words_ids = [np.random.choice(vocab_size, p=probabilities[i]) for i in range(number_of_words)]\n",
    "\n",
    "    return noisy_words_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load GloVe in a specific dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "with open(join(glove_data_folderpath, glove_dimension_to_filename[hidden_size]), \"rb\") as f:\n",
    "    glove = pickle.load(f)\n",
    "\n",
    "vocab_embs = np.array(list(glove.values()))\n",
    "vocab_size = vocab_embs.shape[0]\n",
    "del glove # Save RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select *number_of_words* random words and rank their neighbors according to their distance with the word in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = 5000\n",
    "words_ids = np.random.randint(0, vocab_size, size=number_of_words)\n",
    "words_embs = vocab_embs[words_ids]\n",
    "\n",
    "words_neighbors_ranked = rank_neighbors(words_embs, vocab_embs, distance_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add noise to the embeddings of the words following the $d_x$-privacy mechanism. Apply the post-processing described in the paper and count which neighbor was chosen, represented by its rank in the neighbor list of the initial word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [i for i in range(1, 71)] # Parameter\n",
    "dx_constants = [i for i in np.arange(0.001, 0.1, 0.001)] # Parameter\n",
    "neighbor_counted_occurences = {}\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    embeddings = np.copy(words_embs)\n",
    "    noise = sample_noise_vectors(dimension=hidden_size,\n",
    "                                        shape1=1,\n",
    "                                        shape2=number_of_words,\n",
    "                                        epsilon=epsilon)[0]\n",
    "    # Adding noise to embeddings\n",
    "    noisy_embeddings = embeddings + noise\n",
    "\n",
    "    # We first find the nearest neighbors of each of the noisy embeddings, called the \"pivots\" here\n",
    "    pivot_noisy_word_ids = noisy_embeddings_to_ids(noisy_embeddings, vocab_embs, distance_metric)\n",
    "    pivot_noisy_word_embeddings = vocab_embs[pivot_noisy_word_ids]\n",
    "    \n",
    "    # Then, we apply the post-processing fix proposed in the paper, by sampling a neighbor\n",
    "    # of each pivot according to the formula above. Finally, we count the number of times \n",
    "    # the k-th neighbor has been chosen and store it in neighbor_counted_occurences.\n",
    "    neighbor_counted_occurences[epsilon] = {}\n",
    "\n",
    "    #Rank the words in the vocabulary according to their distance with each of the embeddings\n",
    "    pivot_noisy_word_embeddings_neighbors_ranked = rank_neighbors(pivot_noisy_word_embeddings, vocab_embs, distance_metric)\n",
    "    for dx_constant in dx_constants:\n",
    "        noisy_words_ids = noisy_embeddings_to_ids_dxfix_lazy(pivot_noisy_word_embeddings, vocab_embs, pivot_noisy_word_embeddings_neighbors_ranked, dx_constant, epsilon)\n",
    "\n",
    "        # for all words_ids, get the rank k of noisy_word_ids[i] and increase a counter at index k\n",
    "        noisy_word_ids_ranks = words_neighbors_ranked[np.arange(number_of_words), noisy_words_ids] # This line, for all the elements i in the first dimension of words_neighbors_ranked, gets the particular value pointed by the index which is stored at noisy_word_ids[i]\n",
    "        noisy_word_ids_ranks_counted = Counter(noisy_word_ids_ranks)\n",
    "        neighbor_counted_occurences[epsilon][dx_constant] = [noisy_word_ids_ranks_counted[k] for k in range(vocab_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are stored in *neighbor_counted_occurences*, which is a dictionary where the keys are integers representing the value of epsilon. The dictionary associates each epsilon with another dictionary, where the keys are floats representing the value of the constant $c$ in the post-processing fix. This sub-dictionnary associates each $c$ with a list, where list[i] contains the number of times the i-th neighbor was chosen as the replacement of a word. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dx-privacy-text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
